---
title: "PSY 8712 Week 8 Project"
author: "Ziyu Ren"
date: "2024-04-10"
output:
  html_document:
    df_print: paged
---

## **Script Settings and Resources**
```{r Script Settings and Resources, echo=TRUE, results='hide', message=FALSE, warning=FALSE}
#setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(tidyverse)
library(jsonlite) ## Extract Package
library(tm) 
library(qdap) ## NLP
library(textstem)
library(RWeka) ## set bigram
library(wordcloud) ## graph
library(ldatuning) # LDA
library(topicmodels)
library(tidytext)
library(parallel) # multiple cluster
library(doParallel)
library(psych)
library(caret)
```

## **Data Import and Cleaning**

The following code is usded to extract at least a year of post with jsonlite. 
```{r import data}
# after <- NULL
# posts_list <- data.frame()
# one_year_ago <- (Sys.Date() - 365) %>% as.POSIXct() %>% as.numeric()  # Date one year ago
# 
# # Loop to fetch posts until you reach posts older than one year
# repeat {
#   # Construct the URL
#   url <- paste0('https://oauth.reddit.com/r/iopsychology/new/.json?limit=100', ifelse(is.null(after), '', paste0('&after=', after)))
#   
#   # Make the request
#   response <- fromJSON(url, flatten = TRUE)
#   
#   posts_batch <- response$data$children
#   posts_date <- response$data$children$data.created_utc
#   posts_list <- bind_rows(posts_list, posts_batch)
#   
#   # Check the date of the last post in the batch
#   last_post_date <- max(posts_date)
#   
#   # Update the 'after' parameter for the next request
#   after <- response$data$after
#   
#   # Break the loop if the last post is older than one month
#   if (last_post_date < one_year_ago) {
#     break
#   }
# }

# saveRDS(posts_list, "./R/posts_list.RDS")

## Read in the dataset
posts_list <- readRDS("posts_list.RDS")
```


```{r Make the tbl.}
# Extract title and upvotes
title <- posts_list$data.title
upvotes <- posts_list$data.ups

# save it into week12_tbl
week12_tbl <- tibble(title,upvotes) %>%
  # Remove all / or - otherwise will be treated as same word
   mutate(title = str_replace_all(title,  "-|/", " ")) 
  
```


```{r Compare Function}
compare_them <- function(x, y) {
  casenum <- sample(1:nrow(week12_tbl), 1)
  print(x[[casenum]]$content)
  print(y[[casenum]]$content)
}
```

Create corpus. I choose steps and their sequences mostly based on class recommendations as a good process; for some steps I did not see a lot differences in my output (maybe they are titles so they are abbreviated anyway). However, things like str_to_lower and removePunctuation are very helpful. 
```{r Create corpus.}
io_corpus_original <- VCorpus(VectorSource(week12_tbl$title))
io_corpus <- io_corpus_original %>%
  tm_map(content_transformer(replace_abbreviation)) %>% # replace all abbreviations but not working with M.S., B.A., B.S.
  tm_map(content_transformer(replace_contraction)) %>% # replace contraction with expanded form
  tm_map(content_transformer(str_to_lower)) %>% # replace upper case to lower case
  tm_map(content_transformer(function(x) gsub("[^[:alnum:] ]", "", x))) %>% # remove symbols such as '
  tm_map(removeNumbers) %>% # remove numbers
  tm_map(removePunctuation) %>% # remove punctuation
  tm_map(removeWords, c(stopwords("en"), "I/O", "io", "IO", "psychology", "Psychology", "psy", "psych","iopsychology", "I O")) %>% # remove words that do not convey meaning, I removed amp here because they are from html text and does not convey meaningful info
  tm_map(stripWhitespace) %>% # remove all white spaces
  tm_map(content_transformer(lemmatize_strings)) 


# compare_them(io_corpus_original, io_corpus) 
```


```{r Create DTM.}
# keep bi-grams
myTokenizer <- function(x) {NGramTokenizer(x, Weka_control(min=1, max=2))}
io_dtm_empty <- DocumentTermMatrix(io_corpus, control = list(tokenize = myTokenizer))
# Remove rows with all 0
tokenCounts <- apply(io_dtm_empty, 1, sum)
io_dtm <- io_dtm_empty[tokenCounts > 0, ]
# Save the indices of zero for future reference
indices_zero <- sapply(tokenCounts, function(x) x == 0)
zero_elements <- tokenCounts[indices_zero] %>% 
  names() %>% 
  as.numeric() # Save the number of rows being deleted
# create tibble
io_dtm_tbl <- io_dtm %>% as.matrix() %>% as_tibble() 
io_slim_dtm <- removeSparseTerms(io_dtm, sparse = 0.997) # 794 obs w/ 344 variables, between 2/1 and 3/1
io_slim_dtm_tbl <- io_slim_dtm %>% as.matrix() %>% as_tibble() 
```

## **Visualization**


In general most people are pursuing a job, or doing related research on jobs (mostly with masters).
```{r Wordcloud}
wordcloud(
  words = names(io_dtm_tbl),
  freq = colSums(io_dtm_tbl),
  max.words = 25, # So many words to be plotted otherwise
  colors = brewer.pal(9, "Blues"))
```


## **Analysis**

LDA. Use as many clusters as possible. Both graphs suggest 5 factors, will test from 4 to 6. 
```{r LDA.}
cluster <- makeCluster(5) # R called fatal error with more clusters
registerDoParallel(cluster)

# LDA Tuning
tuning <- FindTopicsNumber(io_dtm, 
                           topics = seq(2,15,1), 
                           metrics = c("Griffiths2004", 
                                       "CaoJuan2009", 
                                       "Arun2010", 
                                       "Deveaud2014"),
                           verbose = T)
FindTopicsNumber_plot(tuning)

stopCluster(cluster)
registerDoSEQ()
```


```{r Based on factors I choose the 5 cluster model.}
# perform LDA
lda_results <- LDA(io_dtm, 5)

# beta matrix
lda_betas <- tidy(lda_results, matrix="beta")
lda_betas_result <- lda_betas %>% 
  group_by(topic) %>% 
  top_n(10, beta) %>% 
  arrange(topic, -beta)

# gamma matrix
lda_gammas <- tidy(lda_results, matrix="gamma")
lda_gammas_result <- lda_gammas %>% 
  group_by(document) %>% 
  top_n(1, gamma) %>% 
  slice(1) %>% 
  ungroup %>% 
  mutate(document = as.numeric(document)) %>% 
  arrange(document)
```



```{r Create topics_tbl}
topics_tbl <- tibble(title) %>% # Use the original title vector
  mutate(doc_id = 1:800) %>% # Create id variable
  filter(!doc_id %in% zero_elements) %>% # Remove rows that are previously removed with zero-elements
  full_join(lda_gammas_result, by = c("doc_id" = "document")) %>% # join gamma results
  rename(original = title, probability = gamma) %>%
  select(doc_id, original, topic, probability) # so don't select upvotes
```

### Answer Questions
1. what topics would you conclude your final topic list maps onto?
Cluster 1: job, interview. Could be how to find a IO related job and tips for interview.
Cluster 2: survey / assessments, career. Creating surveys and assessments, career advices.
Cluster 3: weekly discussions and readings. Maybe this is some weekly thing people do in this subreddit.
Cluster 4: siop and career. Maybe how can one find career opportunities in SIOP. 
Cluster 5: Graduate school and research. Question from graduate students about their research.

2. Do your topic names derived from your interpretation of the beta matrix conceptually match with the content of the original posts? What kind of validity evidence does your answer to this question represent?
Cluster 1 matches, but honestly most posts are talking about careers, so this just happens to capture it.
Cluster 2 matches well. There are some posts in this cluster tlaking about needs assessment and scale development. Mostly still asking for career.
Cluster 3 also gives a clear match. there is something about weekly discusssion.
Cluster 4 and 5 matches normally. Again, most are about graduate students asking about IO career rather than research. 
I think this reflect content/construct validity of our clusters / topics. The content validity of our analysis on topic looks fine. 



```{r Creat final_tbl}
final_tbl <- tibble(title, upvotes) %>% 
  mutate(doc_id = 1:800) %>% # Create id variable
  filter(!doc_id %in% zero_elements) %>% # Remove rows that are previously removed with zero-elements
  full_join(lda_gammas_result, by = c("doc_id" = "document")) %>% # join gamma results
  rename(original = title, probability = gamma) %>%
  select(doc_id, original, upvotes, topic, probability) 
```


For statistical analysis, I perform a anova.
```{r ANOVA}
model <- lm(upvotes ~ topic, data = final_tbl)
summary(aov(model))
```
There is no significant difference among clusters. 

For machine learning analysis, I perform a ols cause I only have one predictor topic, apparently other models need more than that. 
```{r split data}
final_tbl_predict <- final_tbl %>%
  select(upvotes, topic)
## Split Datasets
### Randomly sort row numbers
random_sample <- sample(nrow(final_tbl_predict))
### Shuffle dataset
final_shuffle_tbl <- final_tbl_predict[random_sample, ]
### Find the 75% index
index <- round(nrow(final_tbl_predict) * 0.75)
### Create train data
final_train_tbl <- final_shuffle_tbl[1:index, ]
### Create test data
final_test_tbl <- final_shuffle_tbl[(index+1):nrow(final_tbl_predict), ]
### Create index for 10 foldes
fold_indices <- createFolds(final_train_tbl$upvotes, 10) 
## Set up train control
myControl <- trainControl(method = "cv", # Cross-Validation
                          indexOut = fold_indices, 
                          number = 10,  #10 folds
                          verboseIter = TRUE) ## Printing training log
```


ML: cv_rsq is 0.02, and ho_rsq is 0.00. Topic is not a significant predictor for upvotes, there is no difference (results converge with anova).
```{r ols}
model_ols <- train(upvotes ~ topic, 
                       data = final_train_tbl,
                       method = "lm",  
                       preProcess = "medianImpute", ## Impute Median
                       na.action = na.pass, ## So it will impute
                       trControl = myControl)
ols_predict <- predict(model_ols, final_test_tbl, na.action = na.pass)
cv_rsq <- model_ols$results$Rsquared
ho_rsq <- cor(ols_predict, final_test_tbl$upvotes)^2
```
