---
title: "PSY 8712 Week 8 Project"
author: "Ziyu Ren"
date: "2024-04-10"
output:
  html_document:
    df_print: paged
---

## **Script Settings and Resources**
```{r Script Settings and Resources, echo=TRUE, results='hide', message=FALSE, warning=FALSE}
#setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(tidyverse)
library(jsonlite)
library(tm)
library(qdap)
library(textstem)
library(RWeka)
library(ldatuning)
library(topicmodels)
library(parallel)
library(doParallel)
library(wordcloud)
```

## **Data Import and Cleaning**

The following code is usded to extract at least a year of post with jsonlite. 
```{r}
# after <- NULL
# posts_list <- data.frame()
# one_year_ago <- (Sys.Date() - 365) %>% as.POSIXct() %>% as.numeric()  # Date one year ago
# 
# # Loop to fetch posts until you reach posts older than one month
# repeat {
#   # Construct the URL
#   url <- paste0('https://oauth.reddit.com/r/iopsychology/new/.json?limit=100', ifelse(is.null(after), '', paste0('&after=', after)))
#   
#   # Make the request
#   response <- fromJSON(url, flatten = TRUE)
#   
#   posts_batch <- response$data$children
#   posts_date <- response$data$children$data.created_utc
#   posts_list <- bind_rows(posts_list, posts_batch)
#   
#   # Check the date of the last post in the batch
#   last_post_date <- max(posts_date)
#   
#   # Update the 'after' parameter for the next request
#   after <- response$data$after
#   
#   # Break the loop if the last post is older than one month
#   if (last_post_date < one_year_ago) {
#     break
#   }
# }

# saveRDS(posts_list, "./R/posts_list.RDS")

## Read in the dataset
posts_list <- readRDS("posts_list.RDS")
```

Make the tbl.
```{r}
# Extract title and upvotes
title <- posts_list$data.title
upvotes <- posts_list$data.ups

# save it into week12_tbl
week12_tbl <- tibble(title,upvotes) %>%
  # Remove all IO Psych related stuff with empty space
   mutate(title = str_replace_all(title, "\\b(i/o|io|IO|I/O|I-O|psy|Psy)\\w*\\b", "")) %>%
   mutate(title = str_replace_all(title, "/|-|", " ")) # any / leftover replace with space so later they are not treated the same word
  
```

Compare Function
```{r}
compare_them <- function(x, y) {
  casenum <- sample(1:nrow(week12_tbl), 1)
  print(x[[casenum]]$content)
  print(y[[casenum]]$content)
}
```

Create corpus. I choose steps and their sequences mostly based on class recommendations as a good process; for some steps I did not see a lot differences in my output (maybe they are titles so they are abbreviated anyway). However, things like str_to_lower and removePunctuation are very helpful. 
```{r}
io_corpus_original <- VCorpus(VectorSource(week12_tbl$title))
io_corpus <- io_corpus_original %>%
  tm_map(content_transformer(replace_abbreviation)) %>% # replace all abbreviations but not working with M.S., B.A., B.S.
  tm_map(content_transformer(replace_contraction)) %>% # replace contraction with expanded form
  tm_map(content_transformer(str_to_lower)) %>% # replace upper case to lower case
  tm_map(removeNumbers) %>% # remove numbers
  tm_map(removePunctuation) %>% # remove punctuation
  tm_map(removeWords, c(stopwords("en"), "amp")) %>% # remove words that do not convey meaning, I removed amp here because they are from html text and does not convey meaningful info
  tm_map(stripWhitespace) %>% # remove all white spaces
  tm_map(content_transformer(function(x) gsub("[^[:alnum:] ]", "", x))) %>% # remove symbols such as '
  tm_map(content_transformer(lemmatize_words)) 
  
# compare_them(io_corpus_original, io_corpus) 
```

Create DTM.
```{r}
# keep bi-grams
myTokenizer <- function(x) {NGramTokenizer(x, Weka_control(min=1, max=2))}
io_dtm <- DocumentTermMatrix(io_corpus, control = list(tokenize = myTokenizer))
io_dtm_tbl <- io_dtm %>% as.matrix() %>% as_tibble() 
io_slim_dtm <- removeSparseTerms(io_dtm, sparse = 0.996) # 800 obs w/ 347 variables, between 2/1 and 3/1
io_slim_dtm_tbl <- io_slim_dtm %>% as.matrix() %>% as_tibble() 
```

## **Visualization**

Wordcloud. Receive warnings that 1 word pair (discussion reading) cannot fit so they cannot be plotted given they cannot fit on page. 
In general most people are pursuing a job, or doing related research on jobs (mostly with masters).
It is weired that despite of the lemmatize_words, job and jobs still appear at the same time. A lot of people are holding masters. 
```{r}
wordcloud(
  words = names(io_dtm_tbl),
  freq = colSums(io_dtm_tbl),
  max.words = 25, # So many words to be plotted otherwise
  colors = brewer.pal(9, "Blues"))
```


## **Analysis**

LDA.
```{r}

```
